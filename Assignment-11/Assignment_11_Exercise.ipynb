{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dzNH1ajc3-9"
   },
   "source": [
    "# ECE 57000 Assignment 11 Exercise\n",
    "\n",
    "Your Name:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_WiTpmrQyLlG"
   },
   "source": [
    "For this assignment, you will do an ablation study on the DCGAN model discussed in class and implemented WGAN with weight clipping and (optional) WGAN with gradient penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z029U86108CG"
   },
   "source": [
    "## Exercise 1: Ablation Study on DCGAN\n",
    "An [ablation study](https://en.wikipedia.org/wiki/Ablation_(artificial_intelligence) measures performance changes after changing certain components in the AI system. The goal is to understand the contribution on each component for the overall system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DjmNO7OW6xKp"
   },
   "source": [
    "### Task 1.0 Original DCGAN on MNIST from class note\n",
    "\n",
    "Here is the copy of the code implementation from [course website](https://www.davidinouye.com/course/ece57000-fall-2022/lectures/dcgan-mnist-edit.pdf). Please run the code to obtain the result and **use it as a baseline to compare the results** with the following the ablation tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BffVC_o-A-g6"
   },
   "source": [
    "#### Hyper-parameter and Dataloader setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 516
    },
    "id": "gI6WlRgRboNs",
    "outputId": "86f3f92f-ce37-447d-c97b-40da542d7bff"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "manualSeed = 999\n",
    "#manualSeed = random.randint(1, 10000) # use if you want new results\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmarks = False\n",
    "os.environ['PYTHONHASHSEED'] = str(manualSeed)\n",
    "\n",
    "# Root directory for dataset\n",
    "# dataroot = \"data/celeba\"\n",
    "\n",
    "# Number of workers for dataloader\n",
    "workers = 1\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 128\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "#image_size = 64\n",
    "image_size = 32\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "#nc = 3\n",
    "nc = 1 \n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 100\n",
    "\n",
    "# Size of feature maps in generator\n",
    "#ngf = 64\n",
    "ngf = 8\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "#ndf = 64\n",
    "ndf = 8\n",
    "\n",
    "# Number of training epochs\n",
    "num_epochs = 5\n",
    "num_epochs_wgan = 15\n",
    "num_iters = 250\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0002\n",
    "lr_rms = 5e-4\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Initialize BCELoss function\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Create batch of latent vectors that we will use to visualize\n",
    "#  the progression of the generator\n",
    "fixed_noise = torch.randn(64, nz, 1, 1, device=device)\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1.0\n",
    "fake_label = 0.0\n",
    "\n",
    "# Several useful functions\n",
    "def initialize_net(net_class, init_method, device, ngpu):\n",
    "\n",
    "    # Create the generator\n",
    "    net_inst = net_class(ngpu).to(device)\n",
    "\n",
    "    # Handle multi-gpu if desired\n",
    "    if (device.type == 'cuda') and (ngpu > 1):\n",
    "        net_inst = nn.DataParallel(net_inst, list(range(ngpu)))\n",
    "\n",
    "    # Apply the weights_init function to randomly initialize all weights\n",
    "    #  to mean=0, stdev=0.2.\n",
    "    if init_method is not None:\n",
    "        net_inst.apply(init_method)\n",
    "\n",
    "    # Print the model\n",
    "    print(net_inst)\n",
    "\n",
    "    return net_inst\n",
    "\n",
    "def plot_GAN_loss(losses, labels):\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.title(\"Losses During Training\")\n",
    "\n",
    "    for loss, label in zip(losses, labels):\n",
    "        plt.plot(loss,label=f\"{label}\")\n",
    "\n",
    "    plt.xlabel(\"iterations\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_real_fake_images(real_batch, fake_batch):\n",
    "\n",
    "    # Plot the real images\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Real Images\")\n",
    "    plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "    # Plot the fake images from the last epoch\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Fake Images\")\n",
    "    plt.imshow(np.transpose(fake_batch[-1],(1,2,0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# custom weights initialization called on netG and netD\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "# Download the MNIST dataset\n",
    "dataset = dset.MNIST(\n",
    "    'data', train=True, download=True,\n",
    "   transform=transforms.Compose([\n",
    "       transforms.Resize(image_size), # Resize from 28 x 28 to 32 x 32 (so power of 2)\n",
    "       transforms.CenterCrop(image_size),\n",
    "       transforms.ToTensor(),\n",
    "       transforms.Normalize((0.5,), (0.5,))\n",
    "   ])) \n",
    "\n",
    "# Create the dataloader\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=workers)\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKjBiMDABlfb"
   },
   "source": [
    "#### Architectural design for generator and discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txq0_wTNeXKd"
   },
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is Z, going into a convolution, state size. nz x 1 x 1\n",
    "            nn.ConvTranspose2d( nz, ngf * 4, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True), # inplace ReLU\n",
    "            # current state size. (ngf*4) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # current state size. (ngf*2) x 8 x 8\n",
    "            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # current state size. ngf x 16 x 16\n",
    "            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n",
    "            # current state size. nc x 32 x 32 \n",
    "            # Produce number between -1 and 1, as pixel values have been normalized to be between -1 and 1\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 32 x 32 \n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf) x 16 x 16\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 8 x 8 \n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 4 x 4\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False),\n",
    "            # state size. (ndf*4) x 1 x 1\n",
    "            nn.Sigmoid()  # Produce probability\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0-x9HjzCKJO"
   },
   "source": [
    "#### Loss function and Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zxXFZxfXehWe",
    "outputId": "08e7820d-0419-4858-be03-d12a777b7fe2"
   },
   "outputs": [],
   "source": [
    "# Initialize networks\n",
    "netG = initialize_net(Generator, weights_init, device, ngpu)\n",
    "netD = initialize_net(Discriminator, weights_init, device, ngpu)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ukBlScs0CX_S"
   },
   "source": [
    "#### Visualization of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "zHoLdForfZB5",
    "outputId": "35864c72-e0fc-492c-810b-deef4d97a663"
   },
   "outputs": [],
   "source": [
    "# plot the loss for generator and discriminator\n",
    "plot_GAN_loss([G_losses, D_losses], [\"G\", \"D\"])\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "plot_real_fake_images(next(iter(dataloader)), img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDm-EjTXwUfe"
   },
   "source": [
    "### Task 1.1 Ablation study on batch normalization \n",
    "\n",
    "1. Please modify the code provided in the Task 1.0 so that the neural network architure does not contain any batch normalization layer.  \n",
    "Hint: modify the ***Architectural design for generator and discriminator*** section in Task 1.0\n",
    "2. Train the model with modified networks and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hn03x_rSfZjq",
    "outputId": "5abdff8b-3207-411a-ad49-1859afbfa322"
   },
   "outputs": [],
   "source": [
    "# Generator Code\n",
    "class Generator_woBN(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Generator_woBN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            ################################ YOUR CODE ################################\n",
    "            \n",
    "            ############################# END YOUR CODE ##############################\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "\n",
    "class Discriminator_woBN(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator_woBN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            ################################ YOUR CODE ################################\n",
    "\n",
    "            ############################# END YOUR CODE ##############################\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "netG_noBN = initialize_net(Generator_woBN, weights_init, device, ngpu)\n",
    "netD_noBN = initialize_net(Discriminator_woBN, weights_init, device, ngpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYDnRIgRwi34",
    "outputId": "6f06fa39-68ec-4081-957c-4a5afb646c97"
   },
   "outputs": [],
   "source": [
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD_noBN = optim.Adam(netD_noBN.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG_noBN = optim.Adam(netG_noBN.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD_noBN.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD_noBN(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG_noBN(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD_noBN(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD_noBN.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG_noBN.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD_noBN(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG_noBN.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG_noBN(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "5rmLbZSWwrGm",
    "outputId": "22fa62d6-4fe1-4b6b-8f57-d019c443037a"
   },
   "outputs": [],
   "source": [
    "# plot the loss for generator and discriminator\n",
    "plot_GAN_loss([G_losses, D_losses], [\"G\", \"D\"])\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "plot_real_fake_images(next(iter(dataloader)), img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sXme9r2yw1W0"
   },
   "source": [
    "### Task 1.2 Ablation study on the trick: \"Construct different mini-batches for real and fake\"\n",
    "\n",
    "1. Please modify the code provided in the Task 1.0 so that the discriminator algorithm part computes the forward and backward pass for fake and real images concatenated together (with their corresponding fake and real labels concatenated as well) instead of computing the forward and backward passes for fake and real images separately.  \n",
    "Hint: modify the ***Loss function and Training function*** section in Task 1.0.\n",
    "2. Train the model with modified networks and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fxtAjdxyKHo_",
    "outputId": "91f031f5-d584-42ef-cfd8-7b726de090f8"
   },
   "outputs": [],
   "source": [
    "# re-initilizate networks for the generator and discrimintor.\n",
    "netG = initialize_net(Generator, weights_init, device, ngpu)\n",
    "netD = initialize_net(Discriminator, weights_init, device, ngpu)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "\n",
    "        ################################ YOUR CODE ################################\n",
    "\n",
    "        ############################ END YOUR CODE ##############################\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label = torch.full((b_size,), real_label, device=device)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(G(z)): %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "GAtH0my7xP_y",
    "outputId": "aa0fb21a-cffc-4fcb-d618-617d08671c2e"
   },
   "outputs": [],
   "source": [
    "# plot the loss for generator and discriminator\n",
    "plot_GAN_loss([G_losses, D_losses], [\"G\", \"D\"])\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "plot_real_fake_images(next(iter(dataloader)), img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahUVQpsF5FMg"
   },
   "source": [
    "### Task 1.3 Ablation study on the generator's loss function\n",
    "\n",
    "1. Please modify the code provided in the Task 1.0 so that the *Generator* algorithm part minimizes $\\log(1-D(G(z)))$ instead of the modified loss function suggested in the original GAN paper of $-\\log(D(G(z)))$.\n",
    "    1. Modify the ***Loss function and Training function*** section in Task 1.0 \n",
    "    2. (Hint) Try to understand the definition of [BCE loss](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) first and how the modified loss function was implemented.\n",
    "2. Train the model with modified networks and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2njN7z-XKKaB",
    "outputId": "4741a17d-2ac7-4f2a-86a8-5f4f365031dc"
   },
   "outputs": [],
   "source": [
    "# re-initilizate networks for the generator and discrimintor.\n",
    "netG = initialize_net(Generator, weights_init, device, ngpu)\n",
    "netD = initialize_net(Discriminator, weights_init, device, ngpu)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network\n",
    "        ###########################\n",
    "\n",
    "        ################################ YOUR CODE ################################\n",
    "\n",
    "        ############################ END YOUR CODE ##############################\n",
    "\n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "YP1STZiM6hDb",
    "outputId": "9111cf3d-d579-48da-96c5-ae5f6c64d314"
   },
   "outputs": [],
   "source": [
    "# plot the loss for generator and discriminator\n",
    "plot_GAN_loss([G_losses, D_losses], [\"G\", \"D\"])\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "plot_real_fake_images(next(iter(dataloader)), img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RqfPmYXCsR2"
   },
   "source": [
    "### Task 1.4 Ablation study on the weight initialization\n",
    "\n",
    "1. Please use the function `initialize_net` provided in Task 1.0 to initialize the generator and discriminator function without weight initialization (HINT: There is no need to modify the code for `initialize_net` function).\n",
    "2. Train the model with modified networks and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAxWQTEH8ZQu",
    "outputId": "f600e70a-c2ec-45d0-ce5b-b77c38127bdb"
   },
   "outputs": [],
   "source": [
    "################################ YOUR CODE ################################\n",
    "# netG_woinit = \n",
    "# netD_woinit = \n",
    "###########################  END YOUR CODE ###############################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IQLdVSUCC-wQ",
    "outputId": "7b37b635-d252-43e0-81f1-6172e83716e4"
   },
   "outputs": [],
   "source": [
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD_woinit = optim.Adam(netD_woinit.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG_woinit = optim.Adam(netG_woinit.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ###########################\n",
    "        ## Train with all-real batch\n",
    "        netD_woinit.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD_woinit(real_cpu).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "        errD_real = criterion(output, label)\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG_woinit(noise)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD_woinit(fake.detach()).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "        errD_fake = criterion(output, label)\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD_woinit.step()\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG_woinit.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "        output = netD_woinit(fake).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "        errG = criterion(output, label)\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG_woinit.step()\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG_woinit(fixed_noise).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            \n",
    "        iters += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "wMsX2yJiDzEw",
    "outputId": "3c2329a4-7d4b-48ef-d58a-359fe75f5aa9"
   },
   "outputs": [],
   "source": [
    "# plot the loss for generator and discriminator\n",
    "plot_GAN_loss([G_losses, D_losses], [\"G\", \"D\"])\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "plot_real_fake_images(next(iter(dataloader)), img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47sNuICcD2bC"
   },
   "source": [
    "## Exercise 2: Implement WGAN with weight clipping\n",
    "Wasserstein GAN ([WGAN](https://arxiv.org/abs/1701.07875)) is an alternative training strategy to traditional GAN. WGAN may provide more stable learning and may avoid problems faced in traditional GAN training like mode collapse and vanishing gradient. \n",
    "We will not go through the whole derivation of this algorithm but if interested, you can find more details in the arXiv paper above and [Prof. Inouye's lecture notes on Wasserstein GANs from ECE 570](https://www.davidinouye.com/course/ece57000-fall-2021/lectures/wasserstein-gan.pdf).\n",
    "\n",
    "The **objective function of WGAN** is still a min-max but with a different objective function:\n",
    "$$\n",
    "\\min_G \\max_D \\mathbb{E}_{p_{data}}[D(x)] - \\mathbb{E}_{p_z}[D(G(z))] \\,,\n",
    "$$\n",
    "where $D$ must be a [1-Lipschitz function](https://en.wikipedia.org/wiki/Lipschitz_continuity) (rather than a classifier as in regular GANs) and $p_z$ is a standard normal distribution. Notice the similarities and differences with the original GAN objective:\n",
    "$$\n",
    "\\min_G \\max_D \\mathbb{E}_{p_{data}}[\\log D(x)] + \\mathbb{E}_{p_z}[\\log (1- D(G(z)))] \\,,\n",
    "$$\n",
    "where $D$ is a classifier.\n",
    "Note in practice the WGAN paper uses multiple discriminators (also called \"critics\") so they use multiple $D$s during training.\n",
    "\n",
    "We will not go through the derivation but one approximation **algorithm** for optimizing the WGAN objective is to apply weight clipping to all the weights, i.e., enforce that their absolute value is smaller than some constant $c$. \n",
    "The full pseudo-algorithm can be found on slide 17 in [these slides on WGAN](https://www.davidinouye.com/course/ece57000-fall-2021/lectures/wasserstein-gan.pdf) or in the original paper.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET1R1w6hMhgN"
   },
   "source": [
    "### Your tasks\n",
    "1. Rewrite the loss functions and training function according to the algorithm introduced in slide 17 in [these slides on WGAN](https://www.davidinouye.com/course/ece57000-fall-2021/lectures/wasserstein-gan.pdf). A few notes/hints:\n",
    "    1. Keep the same generator as in Exercise 1, Task 1.0, but modify the discriminator so that there is no restriction on the range of the output. (Simply comment out the last `Sigmoid` layer)\n",
    "    2. Modify the optimizer to be the RMSProp optimizer with a learning rate equal to the value in `lr_rms` (which we set to `5e-4`, which is larger than the rate in the paper but works better for our purposes).\n",
    "    3. Use [`torch.Tensor.clamp_()`](https://pytorch.org/docs/stable/generated/torch.Tensor.clamp_.html) function to clip the parameter values. You will need to do this for all parameters of the discriminator. See algorithm for when to do this.\n",
    "2. Train the model with modified networks and visualize the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few FAQ and hints for the Wasserstein GAN part\n",
    "1. The objective function is different for WGAN. It is simply the mean of the discriminator/critic for real data minus fake data, i.e., for WGAN the value function is $ V(D,G)=\\mathbb{E}_{p_{x \\sim data}}[D(x)] - \\mathbb{E}_{z \\sim p_z}[D(G(z))]$. The expectations can be approximated by empirical expectations (i.e., average over samples in a batch).\n",
    "\n",
    "2. Note that optimizers always assume gradient descent so when optimizing $D$, the loss will be negative of the value function, i.e.,$-V(D,G)$, which is equivalent to gradient ascent on $V(D,G)$.\n",
    "\n",
    "3. For clamping the parameters, you will have to loop over all parameters of `netD.parameters()` and clamp each one in place.\n",
    "\n",
    "4. For the algorithm on slide 17 of the WGAN slides, the function denoted by $f_w$ is equivalent to our $D$ and the function denoted $g_\\theta$ is equivalent to our $G$. The parameters of `netD` are denoted by $w$. Finally, note that in Line 6, it shows gradient ascent (which can be implemented in torch as discussed above), while in line 11, it is using standard gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hARMzCsfD7ik",
    "outputId": "2298ab15-0bc9-48c2-e310-a3a3c35e5d6a"
   },
   "outputs": [],
   "source": [
    "class Discriminator_WGAN(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(Discriminator_WGAN, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        ################################ YOUR CODE ################################\n",
    "\n",
    "        ########################### END YOUR CODE ################################\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n",
    "\n",
    "netG = initialize_net(Generator, weights_init, device, ngpu)\n",
    "netD = initialize_net(Discriminator_WGAN, weights_init, device, ngpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbeRWVv1IAE2",
    "outputId": "b4f559c3-a13d-4e0c-d462-967f03d577d1"
   },
   "outputs": [],
   "source": [
    "############################ YOUR CODE ############################\n",
    "# Setup RMSprop optimizers for both netG and netD with given learning rate as `lr_rms`\n",
    "######################## # END YOUR CODE ##########################\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "n_critic = 5\n",
    "c = 0.01\n",
    "dataloader_iter = iter(dataloader)\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "num_iters = 1000\n",
    "\n",
    "for iters in range(num_iters):\n",
    "    \n",
    "    ###########################################################################\n",
    "    # (1) Train Discriminator more: minimize -(mean(D(real))-mean(D(fake)))\n",
    "    ###########################################################################\n",
    "\n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    for idx_critic in range(n_critic):\n",
    "\n",
    "        netD.zero_grad()\n",
    "\n",
    "        try:\n",
    "            data = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "            data = next(dataloader_iter)\n",
    "\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        D_real = netD(real_cpu).view(-1)\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        D_fake = netD(fake).view(-1)\n",
    "        \n",
    "        ############################ YOUR CODE ############################\n",
    "        # Define your loss function for variable `D_loss`\n",
    "\n",
    "        # Backpropagate the loss function and update the optimizer\n",
    "\n",
    "        # Clip the D network parameters to be within -c and c by using `clamp_()` function\n",
    "        # Note that if all weights are bounded, then the Lipschitz constant is bounded.\n",
    "\n",
    "        ######################## # END YOUR CODE ##########################\n",
    "\n",
    "    ###########################################################################\n",
    "    # (2) Update G network: minimize -mean(D(fake)) (Update only once in 5 epochs)\n",
    "    ###########################################################################\n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    netG.zero_grad()\n",
    "\n",
    "    noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "    fake = netG(noise)\n",
    "    D_fake = netD(fake).view(-1)\n",
    "\n",
    "    ################################ YOUR CODE ################################\n",
    "    # Define your loss function for variable `G_loss`\n",
    "\n",
    "    # Backpropagate the loss function and upate the optimizer\n",
    "\n",
    "    ############################# END YOUR CODE ##############################\n",
    "\n",
    "    # Output training stats\n",
    "    if iters % 10 == 0:\n",
    "        print('[%4d/%4d]   Loss_D: %6.4f    Loss_G: %6.4f'\n",
    "            % (iters, num_iters, D_loss.item(), G_loss.item()))\n",
    "    \n",
    "    # Save Losses for plotting later\n",
    "    G_losses.append(G_loss.item())\n",
    "    D_losses.append(D_loss.item())\n",
    "    \n",
    "    # Check how the generator is doing by saving G's output on fixed_noise\n",
    "    if (iters % 100 == 0):\n",
    "        with torch.no_grad():\n",
    "            fake = netG(fixed_noise).detach().cpu()\n",
    "        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "ZAMyV4SaLrkD",
    "outputId": "966dd153-8a89-42dc-b69e-3791203cee17"
   },
   "outputs": [],
   "source": [
    "# plot the loss for generator and discriminator\n",
    "plot_GAN_loss([G_losses, D_losses], [\"G\", \"D\"])\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "plot_real_fake_images(next(iter(dataloader)), img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqbHcC-rD_Bb"
   },
   "source": [
    "## (Optional and ungraded) Exercise 3: Implement the WGAN with Gradient Penalty\n",
    "1. Use slide 19 in [Lecture note for WGAN](https://www.davidinouye.com/course/ece57000-fall-2021/lectures/wasserstein-gan.pdf) to implement WGAN-GP algorithm. \n",
    "    1. Use the same discriminator and generator as in Exercise 2.\n",
    "    2. Use Adam optimizer for WGAN-GP.\n",
    "    3. If implemented correctly, we have setup some hyperparameters (different than the original algorithm) that seem to work in this situation.\n",
    "    4. For calculating the gradient penalty term, you will need to:\n",
    "        1. Create a batch of interpolated samples.\n",
    "        2. Pass this interpolated batch through the discriminator.\n",
    "        3. Compute the gradient of the discriminator with respect to the samples using [`torch.autograd.grad`](https://pytorch.org/docs/stable/generated/torch.autograd.grad.html). You will need to set:\n",
    "            1. `outputs`\n",
    "            2. `inputs`\n",
    "            3. `grad_outputs`\n",
    "            4. `create_graph=True` and `retain_graph=True` (because we want to backprop through this gradient calculation for the final objective.)\n",
    "            5. Hint: Also make sure to understand the return result of this function to extract the gradients as necessary.\n",
    "        4. Compute the gradient penalty (Hint: For numerical stability, we found that `grad_norm = torch.sqrt((grad**2).sum(1) + 1e-14)` is a simple way to compute the norm.) \n",
    "        5. Use $\\lambda=10$ for the gradient penalty as in the original paper.\n",
    "    \n",
    "3. Train the model with modified networks and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i5son7RLbGgA",
    "outputId": "72ecbc84-be9a-4d80-b629-1858b2c03b3b"
   },
   "outputs": [],
   "source": [
    "# Setup networks for WGAN-GP\n",
    "netG = initialize_net(Generator, weights_init, device, ngpu)\n",
    "netD = initialize_net(Discriminator_WGAN, weights_init, device, ngpu)\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=5e-4, betas=(0.5, 0.9))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=5e-4, betas=(0.5, 0.9))\n",
    "\n",
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "n_critic = 5\n",
    "dataloader_iter = iter(dataloader)\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "num_iters = 1000\n",
    "\n",
    "for iters in range(num_iters):\n",
    "    \n",
    "    ###########################################################################\n",
    "    # (1) Train Discriminator more: minimize -(mean(D(real))-mean(D(fake)))+GP\n",
    "    ###########################################################################\n",
    "\n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = True\n",
    "\n",
    "    for idx_critic in range(n_critic):\n",
    "\n",
    "        netD.zero_grad()\n",
    "\n",
    "        try:\n",
    "            data = next(dataloader_iter)\n",
    "        except StopIteration:\n",
    "            dataloader_iter = iter(dataloader)\n",
    "\n",
    "        real_cpu = data[0].to(device)\n",
    "        b_size = real_cpu.size(0)\n",
    "        D_real = netD(real_cpu).view(-1)\n",
    "\n",
    "        noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        fake = netG(noise)\n",
    "        D_fake = netD(fake).view(-1)\n",
    "        \n",
    "        ############################ YOUR CODE ############################\n",
    "        # Compute the gradient penalty term\n",
    "\n",
    "        # Define your loss function for variable `D_loss`\n",
    "\n",
    "        # Backpropagate the loss function and upate the optimizer\n",
    "\n",
    "        ######################## # END YOUR CODE ##########################\n",
    "\n",
    "    ###########################################################################\n",
    "    # (2) Update G network: minimize -mean(D(fake)) (Update only once in 5 epochs)\n",
    "    ###########################################################################\n",
    "    for p in netD.parameters():\n",
    "        p.requires_grad = False\n",
    "    \n",
    "    netG.zero_grad()\n",
    "\n",
    "    noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "    fake = netG(noise)\n",
    "    D_fake = netD(fake).view(-1)\n",
    "\n",
    "    ################################ YOUR CODE ################################\n",
    "    # Define your loss function for variable `G_loss`\n",
    "\n",
    "    # Backpropagate the loss function and upate the optimizer\n",
    "    \n",
    "    ############################# END YOUR CODE ##############################\n",
    "\n",
    "    # Output training stats\n",
    "    if iters % 10 == 0:\n",
    "        print('[%4d/%4d]   Loss_D: %6.4f    Loss_G: %6.4f'\n",
    "            % (iters, num_iters, D_loss.item(), G_loss.item()))\n",
    "    \n",
    "    # Save Losses for plotting later\n",
    "    G_losses.append(G_loss.item())\n",
    "    D_losses.append(D_loss.item())\n",
    "    \n",
    "    # Check how the generator is doing by saving G's output on fixed_noise\n",
    "    if (iters % 100 == 0):\n",
    "        with torch.no_grad():\n",
    "            fake = netG(fixed_noise).detach().cpu()\n",
    "        img_list.append(vutils.make_grid(fake, padding=2, normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 760
    },
    "id": "Ia3PjuqAOv15",
    "outputId": "b6de94ba-42fd-414f-ca07-b967ef6f4eb1"
   },
   "outputs": [],
   "source": [
    "# plot the loss for generator and discriminator\n",
    "plot_GAN_loss([G_losses, D_losses], [\"G\", \"D\"])\n",
    "\n",
    "# Grab a batch of real images from the dataloader\n",
    "plot_real_fake_images(next(iter(dataloader)), img_list)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment_07_Solution.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "38b65d22d35179c6935a22cd0c80ce3ebfa39e2ed4f084b887887c7215100d32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
