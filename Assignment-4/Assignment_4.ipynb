{"cells":[{"cell_type":"markdown","source":["# ECE 57000 Assignment 4\n","Name:"],"metadata":{"id":"MeW5icxXLysc"},"id":"MeW5icxXLysc"},{"cell_type":"code","execution_count":null,"id":"101f0a32","metadata":{"id":"101f0a32"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_iris"]},{"cell_type":"markdown","id":"b30e8a7d","metadata":{"id":"b30e8a7d"},"source":["# Exercise 1 (20 points)\n","In this exercise, you will optimize a few simple functions where the gradients can be easily manually derived by hand.\n","\n","## Exercise 1.1\n","For each function below, implement the `__call__` method which should evaluate the corresponding function and the `gradient` method which should compute the gradient of the function evaluated at the input $x$. We give a simple linear example ($5x$).\n","\n","1. $5x^2 - 3x + 2$\n","2. $\\exp(x) - x$\n","3. $x - \\log x$ (Note this is only valid for x > 0; in the `__call__` function, raise error if negative value is given by using ValueError with a message \"The input x should be positive.\")"]},{"cell_type":"code","execution_count":null,"id":"ca824f08","metadata":{"id":"ca824f08"},"outputs":[],"source":["class Linear(): # This is the example\n","    def __call__(self, x):\n","        return 5*x\n","\n","    def gradient(self, x):\n","        return 5\n","\n","class Function1(): #\n","    def __call__(self, x):\n","        # your code\n","\n","    def gradient(self, x):\n","        # your code\n","\n","class Function2(): #\n","    def __call__(self, x):\n","        # your code\n","\n","    def gradient(self, x):\n","        # your code\n","\n","class Function3(): #\n","    def __call__(self, x):\n","        # your code\n","\n","    def gradient(self, x):\n","        # your code\n"]},{"cell_type":"code","execution_count":null,"id":"cfec6500","metadata":{"id":"cfec6500"},"outputs":[],"source":["# Evaluation part\n","func1 = Function1()\n","func2 = Function2()\n","func3 = Function3()\n","\n","for x in [2, 0.4, -1]:\n","    if x >= 0:\n","        print(f\"======= x={x} =======\")\n","        print(\"func1 call:\", \"{:.2f}\".format(func1(x)))\n","        print(\"func2 call:\", \"{:.2f}\".format(func2(x)))\n","        print(\"func3 call:\", \"{:.2f}\".format(func3(x)))\n","\n","        print(\"func1 gradient:\", \"{:.2f}\".format(func1.gradient(x)))\n","        print(\"func2 gradient:\", \"{:.2f}\".format(func2.gradient(x)))\n","        print(\"func3 gradient:\", \"{:.2f}\".format(func3.gradient(x)))\n","    else:\n","        print(f\"======= x={x} =======\")\n","        print(\"(The code should raise an error with the message \\\"The input x should be positive.\\\".)\")\n","        gave_err = False\n","        try:\n","            y = func3(x)\n","            print('Did not produce an error for negative')\n","        except Exception as e:\n","            gave_err = True\n","            print('Correctly produced an error for negative')\n","            print(f\"Message: {e}\")"]},{"cell_type":"markdown","id":"6b8dd2aa","metadata":{"id":"6b8dd2aa"},"source":["## Exercise 1.2\n","Implement gradient descent on $x$ to numerically find the minimum of these functions starting at $x=1.5$ with a step size of 0.1. In the iteration loop, **print the updated x value every 10 iteration.**"]},{"cell_type":"code","execution_count":null,"id":"eadb81fa","metadata":{"id":"eadb81fa"},"outputs":[],"source":["starting_point = 1.5\n","step_size = 0.1\n","n_iter = 50\n","\n","print(\"====== func1 ======\")\n","x = starting_point\n","\n","# your code\n","\n","print(f\"minimum of func1 after {n_iter} iterations:\", \"{:.2f}\".format(func1(x)))\n","print(\"\\n\")\n","\n","print(\"====== func2 ======\")\n","x = starting_point\n","\n","# your code\n","\n","print(f\"minimum of func2 after {n_iter} iterations:\", \"{:.2f}\".format(func2(x)))\n","print(\"\\n\")\n","\n","print(\"====== func3 ======\")\n","x = starting_point\n","\n","# your code\n","\n","print(f\"minimum of func3 after {n_iter} iterations:\", \"{:.2f}\".format(func3(x)))"]},{"cell_type":"markdown","id":"5691b679","metadata":{"id":"5691b679"},"source":["# Exercise 2\n","In this exercise, you will optimize and compare different versions of binary classifiers using variants of gradient descent.\n","\n","## Exercise 2.1 (30 points)\n","You will implement the several classification objectives, corresponding gradients and prediction functions. These will be the fundamental building blocks for creating your gradient descent algorithms in the next exercise.\n","\n","We will generalize logistic regression to a more general framework for linear classification models. We will break the problem down into two parts. The first part is the linear projection part (i.e., reduce to a single dimension) that can be formalized as $\\hat{z} = \\theta^T \\mathbf{x}$, where $\\mathbf{x} \\in \\mathbb{R}^{d+1}$ is a instance vector of length $d$ where we concatenate with a 1 to account for the intercept/bias term.\n","\n","The second part is applying a loss function $\\ell(y,\\hat{z})$ that computes a loss given the true label $y$ and a predicted \"score\" $z\\in\\mathbb{R}$ from the first step. You will implement 3 different versions of $\\ell$ in the `__call__` method of each class (Note: You will need to compute $\\hat{z}$ first from the input $\\mathbf{x}$ and model parameters $\\theta$ and then apply this loss function).\n","\n","1. $\\ell(y,\\hat{z}) = (y-\\hat{z})^2$ (Squared error used in ordinary least squared linear regression)\n","2. $\\ell(y,\\hat{z}) = -y \\log \\sigma(\\hat{z}) - (1-y)\\log (1-\\sigma(\\hat{z}))$ (Logistic regression loss)\n","3. $\\ell(y,\\hat{z}) = \\max \\{0, 1-(2y-1)\\hat{z}\\}$ (Hinge loss of support vector machines (SVM) modified for $y\\in \\{0,1\\}$)\n","\n","The gradients for each of these w.r.t. a single sample $\\mathbf{x}$ should be implemented in the `gradient` method:\n","\n","1. $\\nabla_{\\theta} \\ell(y, \\theta^T \\mathbf{x}) = 2(y-\\theta^T\\mathbf{x})(-\\mathbf{x}) = -2(y-\\theta^T\\mathbf{x})\\mathbf{x}$\n","2. $\\nabla_{\\theta} \\ell(y, \\theta^T \\mathbf{x}) = -(y-\\sigma(\\theta^T\\mathbf{x})) \\mathbf{x}$\n","3. $\\nabla_{\\theta} \\ell(y, \\theta^T \\mathbf{x}) = \\begin{cases} -(2y-1)\\mathbf{x}, & \\text{if } (2y-1)\\theta^T\\mathbf{x} < 1 \\\\ \\textbf{0}, & \\text{otherwise} \\end{cases}$\n","\n","Finally, for each loss function, there is a different threshold for $\\hat{z}$ to predict class 1 vs class 0. We give the predicted class given only $\\hat{z}$ for each loss function, which should be implemented in the `predict` method:\n","\n","1. $\\hat{y} = \\begin{cases} 1, & \\text{if } \\hat{z} \\geq 0.5 \\\\ 0, & \\text{otherwise} \\end{cases}$\n","2. $\\hat{y} = \\begin{cases} 1, & \\text{if } \\hat{z} \\geq 0, \\text{ (or equivalently } \\sigma(\\hat{z}) \\geq 0.5 \\text{)} \\\\ 0, & \\text{otherwise} \\end{cases}$\n","3. $\\hat{y} = \\begin{cases} 1, & \\text{if } \\hat{z} \\geq 0 \\\\ 0, & \\text{otherwise} \\end{cases}$\n","\n","Implement each of these in a **vectorized way** (without for loops) such that it can take in a parameter vector $\\theta$ called `theta`, the input value matrix $X\\in\\mathbb{R}^{n \\times (d+1)}$ called `X`, and the corresponding label vector $y$ called `y` (except the predict method which should only take in the parameters and input). Note that in almost all cases, you will need to compute $\\hat{z} = \\theta^T \\mathbf{x}$ (or in vectorized form for multiple samples at once $\\hat{\\mathbf{z}} = X\\theta$) as a first step before doing further calculations. A few further hints:\n","\n","Hint 1: You will need to use numpy broadcasting rules to implement in a vectorized way. Please read  https://numpy.org/doc/stable/user/basics.broadcasting.html to understand broadcasting rules. This will enable you to avoid loops in your code. In particular, for the gradient calculations, you will need to apply a scaling to each row of `X` before taking an average to get the final gradient.  Suppose you have a scaling 1D array (i.e., vector) `a` with shape `(n,)` and you want to scale each row of `X` which has shape `(n,D)`. You cannot do `a * X` because the rightmost dimensions do not match and neither is 1 (see broadcasting documentation).  Instead, you can do **`a.reshape(-1, 1) * X`** (see https://numpy.org/doc/stable/reference/generated/numpy.ndarray.reshape.html). This will fit the broadcasting rules because after the reshape because `a.reshape(-1,1)` will now have shape `(n,1)` and `X` has shape `(n,D)` so the broadcasting rules will apply and the scaling will be applied to each row of `X`.\n","\n","Hint 2: When you take a mean of the per-sample gradients, you should make sure to use the `axis` parameter to take a mean over a particular axis. Without the `axis` parameter, the mean will take a mean over the entire matrix yielding a scalar value.\n","\n","Hint 3: For the `predict` function, you can just use the `>=` operator on a whole vector. This will produce a boolean vector with only True and False values (or equivalently 0 and 1 values).\n","\n","Hint 4: For the gradient of the hinge loss, you can use a boolean mask for rows that satisfy the condition and use it to filter only the non-zero gradients."]},{"cell_type":"code","execution_count":null,"id":"3aeef819","metadata":{"id":"3aeef819"},"outputs":[],"source":["class SquaredError():\n","    def __call__(self, theta, X, y):\n","        # your code (Should return a scalar loss value)\n","\n","    def gradient(self, theta, X, y):\n","        # your code (Should return a gradient vector the same shape as theta)\n","\n","    def predict(self, theta, X):\n","        # your code (Should return a vector of predictions for each row of X)\n","\n","\n","class LogisticLoss():\n","    def __init__(self):\n","        self.sigmoid = lambda x: 1 / (1 + np.exp(-x)) # For your convenience.\n","\n","    def __call__(self, theta, X, y):\n","        # your code\n","\n","    def gradient(self, theta, X, y):\n","        # your code\n","\n","    def predict(self, theta, X):\n","        # your code\n","\n","\n","class HingeLoss():\n","    def __call__(self, theta, X, y):\n","        # your code\n","\n","    def gradient(self, theta, X, y):\n","        # your code (No for loop here)\n","\n","    def predict(self, theta, X):\n","        # your code"]},{"cell_type":"code","execution_count":null,"id":"6379e22e","metadata":{"id":"6379e22e"},"outputs":[],"source":["X, y = load_iris(return_X_y=True)\n","\n","# It has 3 classes: 0, 1, 2. We are going to use only two classes.\n","X = X[y<2]\n","y = y[y<2]\n","n, D = X.shape\n","\n","# Increase one more dimension of X for interecept term,\n","#  i.e., (n, D) => (n, D+1).\n","X = np.concatenate((X, np.ones((n, 1))), axis=1)\n","rng = np.random.RandomState(0)\n","theta = rng.randn(D + 1)\n","\n","# Evaluation\n","for f in [SquaredError(), LogisticLoss(), HingeLoss()]:\n","    loss = f(theta, X, y)\n","    grad = f.gradient(theta, X, y)\n","    idx = [1,2,3,-1,-2,-3]\n","    pred = f.predict(theta, X[idx, :])\n","    print(f\"{f.__class__.__name__} : {loss}\")\n","    print(f\"Gradient: {grad}\")\n","    print(\"Predictions for first 3 and last 3 (might be all 1s): \", pred.astype(int) )\n","    print(f\"Gradient shape correct? {np.all(grad.shape == theta.shape)}\")\n","    print(f\"Prediction shape correct? {len(pred.shape) == 1 and np.all(pred.shape[0] == len(idx))}\\n\")"]},{"cell_type":"markdown","id":"e6617ef1","metadata":{"id":"e6617ef1"},"source":["## Exercise 2.2 (50 points)\n","Given your objective and gradient implementations above, implement mini-batch GD in the following function (if `batch_size` is `None`, this defaults to GD). You should track the average objective and training accuracy after every iteration and append to `obj_arr` and `acc_arr` respectively. The average objective in each iteration can be calculated by taking the average over batches of the mean losses for each batch. Accuracy can be obtained based on the total number of correct predictions in each iteration over all the batches. Return the best theta (in terms of accuracy). We have provided code to shuffle and batch the input data, a skeleton for the optimize function, and evaluation code to plot your results. See code comments for a few more details.\n","\n","You should expect to get high accuracy and low loss values after 100 epochs if you have implemented the objectives, gradients, and optimize function correctly. The expected accuracy and loss values for the first and last epoch in the case of SquaredError are provided in the comments as a reference."]},{"cell_type":"code","execution_count":null,"id":"4a99b10d","metadata":{"id":"4a99b10d"},"outputs":[],"source":["def shuffle_and_batch(X, y, batch_size, rng):\n","    \"\"\"Splits both X and y into nearly equal batches\"\"\"\n","    assert X.shape[0] == y.shape[0], 'X and y should have the same number of elements'\n","    # Shuffle data\n","    shuffled_idx = rng.permutation(X.shape[0])\n","    X = X[shuffled_idx, :]\n","    y = y[shuffled_idx]\n","    # Split into batches based on batch_size\n","    X_batches = np.asarray(np.array_split(X, np.ceil(X.shape[0] / batch_size), axis=0))\n","    y_batches = np.asarray(np.array_split(y, np.ceil(y.shape[0] / batch_size), axis=0))\n","    return X_batches, y_batches"]},{"cell_type":"code","execution_count":null,"id":"442056bb","metadata":{"id":"442056bb"},"outputs":[],"source":["def optimize(theta_init, X_raw, y_raw, obj_func, step_size=1,\n","             max_epoch=100, batch_size=None, rng = None):\n","    obj_arr = []\n","    acc_arr = []\n","    batch_size = batch_size if batch_size is not None else len(X_raw)\n","\n","    if rng is None:\n","        rng = np.random.RandomState(0)\n","\n","    theta = theta_init.copy()\n","    best_acc = 0\n","    best_theta = theta\n","    for i in range(max_epoch): # epoch\n","        # Create list of batches for both X and y,\n","        # X_batches[0] has shape (batch_size, D) and y_batches[0] has shape (batch_size,)\n","        X_batches, y_batches = shuffle_and_batch(X_raw, y_raw, batch_size, rng)\n","\n","        loss_for_each_epoch = 0 # total loss for the epoch\n","        num_correct = 0 # number of correct predictions for the epoch\n","        ######## Start your code ########\n","        # Loop through batches, update theta,\n","        #  and keep a running loss and running count of correct to\n","        #  calculate average loss and accuracy after each epoch\n","\n","\n","        # After each pass through the data (i.e., an epoch),\n","        #  save average objective and accuracy for the epoch,\n","        #  and update the best theta if needed (i.e., if\n","        #  current theta is better than best in terms of accuracy)\n","\n","\n","        ######## End your code ########\n","        # Display average objective and accuracy for the first and the last epoch\n","        if i == 0 or i == max_epoch - 1:\n","            print(f'Epoch: {i+1}, Average Loss: {obj_arr[i]}, Accuracy: {acc_arr[i]}')\n","\n","    return best_theta, obj_arr, acc_arr"]},{"cell_type":"code","execution_count":null,"id":"6be6ebbb","metadata":{"id":"6be6ebbb"},"outputs":[],"source":["# Code to run algorithm and plot the loss/accuracy\n","# Step sizes have been preselected to be reasonable\n","obj_func_arr = [SquaredError(), LogisticLoss(), HingeLoss()]\n","step_sizes = [\n","    [5e-4, 1e-4],\n","    [5e-2, 1e-2],\n","    [1e-2, 5e-2],\n","]\n","\n","# Intialize random number generator\n","rng = np.random.RandomState(42)\n","\n","for obj_func, step_size_arr in zip(obj_func_arr, step_sizes): # 0.005, 0.001 for SquaredError, LogisticLoss, HingeLoss\n","    print(f'======= {obj_func.__class__.__name__} =======')\n","    theta_init = rng.randn(D + 1)\n","\n","    print(f'-> Running Gradient Descent')\n","    best_theta, obj_arr, acc_arr = optimize(\n","        theta_init, X, y, obj_func,\n","        step_size=step_size_arr[0], max_epoch=100, batch_size=None, rng = rng)\n","    print(f'\\nBest theta: {best_theta}\\n')\n","\n","    print(f'-> Running Mini-Batch Gradient Descent (Batch Size = 10)')\n","    best_theta_sgd, obj_arr_sgd, acc_arr_sgd = optimize(\n","        theta_init, X, y, obj_func,\n","        step_size=step_size_arr[1], max_epoch=100, batch_size=10, rng = rng)\n","    print(f'\\nBest theta_sgd: {best_theta_sgd}')\n","    print('')\n","\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,2.5))\n","    ax1.set_title(f\"Loss plot of {obj_func.__class__.__name__}\")\n","    ax1.set(xlabel=\"Epoch\", ylabel=\"Loss\")\n","    ax1.plot(np.arange(1, len(obj_arr)+1), obj_arr_sgd, color =\"red\", label=\"SGD\")\n","    ax1.plot(np.arange(1, len(obj_arr)+1), obj_arr, color =\"blue\", label=\"GD\")\n","    ax1.legend(loc=\"upper left\")\n","    ax2.set_title(f\"Acc plot of {obj_func.__class__.__name__}\")\n","    ax2.set(xlabel=\"Epoch\", ylabel=\"Accuracy\")\n","    ax2.plot(np.arange(1, len(acc_arr)+1), acc_arr_sgd, color =\"red\", label=\"SGD\")\n","    ax2.plot(np.arange(1, len(acc_arr)+1), acc_arr, color =\"blue\", label=\"GD\")\n","    ax2.legend(loc=\"upper left\")\n","\n","######## Expected output for SquaredError ########\n","# ======= SquaredError =======\n","# -> Running Gradient Descent:\n","# Epoch: 1, Average Loss: 21.53751788972237, Accuracy: 0.5\n","# Epoch: 100, Average Loss: 0.6446394592527506, Accuracy: 1.0 ...\n","#\n","# -> Running Mini-Batch Gradient Descent:\n","# Epoch: 1, Average Loss: 21.400022318225002, Accuracy: 0.5\n","# Epoch: 100, Average Loss: 0.4496070478822987, Accuracy: 1.0 ..."]}],"metadata":{"kernelspec":{"display_name":"Python 3.10.6 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}